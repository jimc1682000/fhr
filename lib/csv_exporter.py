import csv
import os
from collections import OrderedDict
from collections.abc import Iterable


def _header_row(incremental_mode: bool) -> list[str]:
    headers = ['æ—¥æœŸ', 'é¡žåž‹', 'æ™‚é•·(åˆ†é˜)', 'èªªæ˜Ž', 'æ™‚æ®µ', 'è¨ˆç®—å¼']
    if incremental_mode:
        headers.append('ç‹€æ…‹')
    return headers


def _status_row(last_date: str, complete_days: int, last_analysis_time: str) -> list[str]:
    return [
        last_date,
        "ç‹€æ…‹è³‡è¨Š",
        0,
        (
            f"ðŸ“Š å¢žé‡åˆ†æžå®Œæˆï¼Œå·²è™•ç†è‡³ {last_date}ï¼Œå…± {complete_days} å€‹å®Œæ•´å·¥ä½œæ—¥ | "
            f"ä¸Šæ¬¡åˆ†æžæ™‚é–“: {last_analysis_time}"
        ),
        "",
        "ä¸Šæ¬¡è™•ç†ç¯„åœå…§ç„¡æ–°å•é¡Œéœ€è¦ç”³è«‹",
        "ç³»çµ±ç‹€æ…‹",
    ]


def _issue_row(issue, incremental_mode: bool) -> list[str]:
    row = [
        issue.date.strftime('%Y/%m/%d'),
        issue.type.value,
        issue.duration_minutes,
        issue.description,
        issue.time_range,
        issue.calculation,
    ]
    if incremental_mode:
        row.append("[NEW] æœ¬æ¬¡æ–°ç™¼ç¾" if getattr(issue, 'is_new', False) else "å·²å­˜åœ¨")
    return row


def write_headers(writer: csv.writer, incremental_mode: bool) -> None:
    writer.writerow(_header_row(incremental_mode))


def write_status_row(
    writer: csv.writer, last_date: str, complete_days: int, last_analysis_time: str
) -> None:
    writer.writerow(_status_row(last_date, complete_days, last_analysis_time))


def write_issue_rows(writer: csv.writer, issues: Iterable, incremental_mode: bool) -> None:
    for issue in issues:
        writer.writerow(_issue_row(issue, incremental_mode))


def _build_rows(
    issues: list[object], incremental_mode: bool, status: tuple | None = None
) -> list[list[str]]:
    rows: list[list[str]] = []
    rows.append(_header_row(incremental_mode))
    if status and incremental_mode and not issues:
        last_date, complete_days, last_analysis_time = status
        rows.append(_status_row(last_date, complete_days, last_analysis_time))
    for issue in issues:
        rows.append(_issue_row(issue, incremental_mode))
    return rows


def _normalize_row(row: list[str], width: int) -> list[str]:
    if len(row) > width:
        return row[:width]
    if len(row) < width:
        return row + [''] * (width - len(row))
    return row


def _row_key(row: list[str]) -> tuple:
    if len(row) > 1 and row[1] == 'ç‹€æ…‹è³‡è¨Š':
        return ('STATUS', row[0])
    limit = min(6, len(row))
    return tuple(row[:limit])


def _merge_rows(existing: list[list[str]], new_rows: list[list[str]]) -> list[list[str]]:
    """Merge existing CSV rows with new rows, deduplicating by key.

    New rows take precedence over existing ones with the same key.
    Status rows are always positioned after the header.
    """
    header = new_rows[0]
    width = len(header)
    merged = OrderedDict()

    # First, add existing rows (excluding header)
    if existing:
        for row in existing[1:]:  # Skip existing header
            if not row:
                continue
            normalized = _normalize_row(row, width)
            merged[_row_key(normalized)] = normalized

    # Then add/update with new rows (they take precedence)
    for row in new_rows[1:]:
        if not row:
            continue
        normalized = _normalize_row(row, width)
        merged[_row_key(normalized)] = normalized

    result: list[list[str]] = [header]

    # Extract status row if present
    status_key = None
    for key in list(merged):
        if key and key[0] == 'STATUS':
            status_key = key
            break

    # Place status row immediately after header
    if status_key is not None:
        result.append(merged.pop(status_key))

    # Add remaining rows
    result.extend(merged.values())
    return result


def save_csv(
    filepath: str,
    issues: Iterable,
    incremental_mode: bool,
    status: tuple | None = None,
    merge: bool = False,
) -> None:
    """Persist CSV with optional status row.

    status: (last_date, complete_days, last_analysis_time) if provided
    merge: when True, rewrite the canonical export in place (no timestamped backup).
    """

    issues_list = list(issues)
    rows = _build_rows(issues_list, incremental_mode, status)

    if merge and os.path.exists(filepath):
        # Read existing CSV data for merging
        existing_rows: list[list[str]] = []
        try:
            with open(filepath, 'r', encoding='utf-8-sig') as f:
                reader = csv.reader(f, delimiter=';')
                existing_rows = list(reader)
        except (FileNotFoundError, IOError):
            # If file doesn't exist or can't be read, proceed with new rows only
            existing_rows = []
        rows = _merge_rows(existing_rows, rows)

    with open(filepath, 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.writer(f, delimiter=';')
        writer.writerows(rows)
